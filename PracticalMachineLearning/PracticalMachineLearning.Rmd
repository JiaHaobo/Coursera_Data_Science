---
title: "Practical Machine Learning"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    keep_md: yes
    theme: cerulean
---

## Background  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The data are collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

In this project, machine learning techniques are employed to predict the the manners by the fitted model and data from accelerometers.

## Getting and Cleanning The Data
The training data for this project are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Since (1) there are missing data in the raw data set; (2) the data in the type of character is not proper for fitting the model; (3) not all the information in the raw data is useful for fitting the model, the raw data need to be cleaned by the following steps before further analysis:  
  1. Only the variables that are available for all the observations are chosed for model fitting.
  2. Remove the identity, time and window information from the raw data set.
  3. Change all the data into **numetric** data type.
```{r,eval=TRUE}
library(caret)
# Getting the data
data.set.train <- read.csv("pml-training.csv")
data.set.test <- read.csv("pml-testing.csv")

# Cleaning the data
## Only choose the variables that are available for all the observations.
# data.set.train
No.na.train <- apply(data.set.train,2,function(x) length(which(is.na(x)))) 
valid.var.train <- names(No.na.train)[which(No.na.train == 0)]
# data.set.test
No.na.test <- apply(data.set.test,2,function(x) length(which(is.na(x)))) 
valid.var.test <- names(No.na.test)[which(No.na.test == 0)]
valid.var <- c(intersect(valid.var.train,valid.var.test),"classe")

## Remove the identity, time and window information from the dataset.
data.set.train <- data.set.train[valid.var][-(1:7)]
data.set.test <- data.set.test[head(valid.var,-1)][-(1:7)]
## Change all the data into numetric type
for(i in 1:(ncol(data.set.train)-1)) data.set.train[,i] <- as.numeric(data.set.train[,i])
for(i in 1:(ncol(data.set.test)-1)) data.set.test[,i] <- as.numeric(data.set.test[,i])
dim(data.set.train)
```
The final data set includes 19622 observations of 53 variables. 

## Model Fitting

### Data Partition for Training and Testing
I order to check the prediction ability, the data is splited to two subset.
  1. A subset with 75% of observations is used for model fitting.
  2. A subset with 25% of observations is used for validation.

```{r,eval=TRUE}
# Model Fitting
set.seed(12345)
## Create the data partition for cross validation
inTraining <- createDataPartition(data.set.train$classe,p=0.75,list = FALSE)
training <- data.set.train[inTraining,]
testing <- data.set.train[-inTraining,]
data.frame(Train=nrow(training),Test=nrow(testing),row.names="Number of Observation")
```

### Pre-settings for the Model Fitting
In order to avoid over-fitting and achieve a better prediction ability, we use the cross-validation technique for fitting. In the following analysis, 4-fold cross-validation is employed.

```{r,eval=TRUE}
## Fit control: 4-fold cross validation
fitControl <- trainControl(method = "cv", 
                           number = 4,
                           repeats = 1,
                           horizon = 1,
                           verboseIter = FALSE,
                           returnData = FALSE,
                           returnResamp = "none",
                           savePredictions = TRUE,
                           classProbs = TRUE,
                           summaryFunction = defaultSummary,
                           selectionFunction = "best",
                           predictionBounds = rep(FALSE, FALSE),
                           seeds = NA,
                           allowParallel = TRUE)
```

### Fit the Random Forest Model
```{r,eval=TRUE}
# Fit the random forest model
fit.RF <- train(classe ~ ., data=training, method="rf", trControl = fitControl,ntree=300, 
                tuneGrid =data.frame(mtry=27) ,preProcess = NULL)
fit.RF
```

#### Fit Quality
We can check the model fitting quality by confution matrix, the fit quality is really good in this case, with an accuracy of 100%.

```{r,eval=TRUE}
# Confusion Matrix
cM.train.RF <- confusionMatrix(predict(fit.RF,training),training$classe)
cM.train.RF
```

#### Model Validation
The out-of-sample accuracy for the model is very important. It stands for the predictability for a model. We can check the out-of-sample quality by confusion matrix of the testing data.

```{r, eval=TRUE}
cM.test.RF  <- confusionMatrix(predict(fit.RF,testing),testing$classe)
cM.test.RF
```

### Fit the C5.0Tree Model
```{r, eval=TRUE}
# Fit the C5.0Tree model
fit.C5T <- train(classe ~ ., data=training, method="C5.0Tree", trControl = fitControl,preProcess = NULL)
fit.C5T
```

#### Fit Quality
We can check the model fitting quality by confution matrix, the fit quality is really good in this case, with an accuracy of 99%.

```{r, eval=TRUE}
# Confusion Matrix
cM.train.C5T <- confusionMatrix(predict(fit.C5T,training),training$classe)
cM.train.C5T
```

#### Model Validation
The out-of-sample accuracy for the model is very important. It stands for the predictability for a model. We can check the out-of-sample quality by confusion matrix of the testing data.

```{r, eval=TRUE}
cM.test.C5T <- confusionMatrix(predict(fit.C5T,testing),testing$classe)
cM.test.C5T
```

### Conclusion
**The random forest model over-perform the C5.0Tree Model.** We choose Random Forest model for the prediction on the testing data set for assignment 2.

## Predict the Testing Data for Assignment 2 by Chosen Model
```{r, eval=TRUE}
model <- fit.RF
answer <- predict(model,newdata=data.set.test)
answer <- as.character(answer)
answer
```

```{r,eval=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answer)
```
